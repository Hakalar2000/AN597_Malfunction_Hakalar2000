---
title: "Malfunction"
author: "Me"
date: "October 19, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. Write a simple R function, Z.prop.test(), that can perform one- or two-sample Z-tests for proportion data, using the following guidelines:
*Your function should take the following arguments: p1 and n1 (no default) representing the estimated proportion and sample size (i.e., based on your sample data); 
*p2 and n2 (both defaulting to NULL) that contain a second sample’s proportion and sample size data in the event of a two-sample test; 
*p0 (no default) as the expected value for the population proportion; 
*alternative (default “two.sided”) and conf.level (default 0.95), to be used in the same way as in the function t.test().
*When conducting a two-sample test, it should be p1 that is tested as being smaller or larger than p2 when alternative=“less” or alternative=“greater”, the same as in the use of x and y in the function t.test().
*The function should perform a one-sample Z-test using p1, n1, and p0 if either p2 or n2 (or both) is NULL.
*The function should contain a check for the rules of thumb we have talked about (n∗p>5 and n∗(1−p)>5 to ensure the validity of assuming the normal distribution in both the one- and two-sample settings. If this is violated, the function should still complete but it should also print an appropriate warning message.
*The function should return a list containing the members Z (the test statistic), P (the appropriate p value), and CI (the two-sided CI with respect to “conf.level” around p1 in the case of a one-sample test and around p2-p1 in the case of a two-sample test). For all test alternatives (“two.sided”, “greater”, “less”), calculate symmetric CIs based on quantiles of the normal distribution rather than worrying about calculating single-limit confidence bounds.

```{r}
#Let us call make some data of 0 and 1. 0 being FALSE and 1 being TRUE
install.packages("wakefield")
library(wakefield) #this package will let us make random data using True or False
logic<-r_sample_logical(100, name = "Var")
str(logic)
#now we write our function so the journey begins
#I know the basic structure of a function:
name<-function(argument){
  Statement
}
#Now I need to add in a bunch of statement stuff so I can use it on any proportional data set. 

z.prop.test<-function(x){
  one.sample<-binom.test(x, n, p = 0.05, alternative = "two.sided")
  result <- paste(one.sample)
 return(result)
}
z.prop.test(logic)
```


2. The dataset from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict longevity (MaxLongevity_m) measured in months from species’ brain size (Brain_Size_Species_Mean) measured in grams. Do the following for both longevity~brain size and log(longevity)~log(brain size):
#Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the the fitted model equation to your plot (HINT: use the function geom_text()).
#Identify and interpret the point estimate of the slope (β1), as well as the outcome of the test associated with the hypotheses H0: β1 = 0; HA: β1 ≠ 0. Also, find a 90 percent CI for the slope (β1) parameter.
#Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines.
#Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?
#Looking at your two models, which do you think is better? Why?


```{r}
#First lets load out packages that we will need:
library(curl)
library(ggplot2)
install.packages("ggpmisc")
library(ggpmisc) #for adding equations to my graph
library(gridExtra)
```

```{r}
#Next load in data:
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall19/KamilarAndCooperData.csv")
t <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
why<-na.omit(t) #gotta get rid of NA values
d<-data.frame(why) #gotta put our data into a data.frame to work with ggplot
x<-d$MaxLongevity_m #time variable is going to be x
y<-d$Brain_Size_Species_Mean #dependent variable is y
r<-cbind(x, y) #I will need this later on when combining dataframes. 
```

Lets start with our regular regression
```{r}
#Now we can work out the problem 

#with the lm() function, we can calculate  Model I and plot regression

Mod<-lm(data = d, y~x) #not hard, but note that the lm() function yeilds the OSL regression and there are other types we could have tried but were to lazy to do. 
Mod
summary(Mod)

#Ok regression coefficients beta1 and beta0
  #x is life
  #y is size
beta1 <- cor(x, y) * (sd(x)/sd(y))
beta1
beta0 <- mean(x) - beta1 * mean(y)
beta0
ci.slope<-confint(Mod, level = 0.9)
ci.slope #x parameter of the output gives you the upper and lower CIs. 
#please note that a CI of 90% corresponds to values between the lower 5% and upper 95%. 

#Now we use ggplot and the package we pulled from le internet to do the do

gMod1<-ggplot(data = d, aes(x = x, y = y)) + geom_point() + geom_smooth(method = "lm", formula = y ~ x) + stat_poly_eq(formula = Mod, aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~")), parse = TRUE)
gMod1
#I have no idea why it comes out with a parabolic equation...

Mod
summary(Mod)
ci <- predict(Mod, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "confidence", 
    level = 0.90)  # CIs for a vector of values
head(ci)
ci.frame<-data.frame(ci) #had to change these to dataframe to combine them later on
pi <- predict(Mod, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "prediction", 
    level = 0.90)  # for a vector of values
head(pi)
pi.frame<-data.frame(pi)
New<-cbind(r, ci.frame, pi.frame)
names(New) <- c("x", "y", "CIfit", "CIlwr", "CIupr", "PIfit", "PIlwr", "PIupr") #renaming columns
head(New)

gModII<-ggplot(data = New, aes(x = x, y = y)) + geom_point() + 
  geom_line(data = New, aes(x = x, y = CIfit), colour = "black") +
  geom_line(data = New, aes(x = x, y = CIlwr), colour = "blue") +
  geom_line(data = New, aes(x = x, y = CIupr), colour = "blue") +
  geom_line(data = New, aes(x = x, y = PIlwr), colour = "red") +
  geom_line(data = New, aes(x = x, y = PIupr), colour = "red")
gModII

#To whomever has to give me commentary. Idk why, but when I first ran my code, my CIs and PIs were not working at all for some reason and then they magically decided to now work...Idk man...its magic. 


```
Now lets do our log transformed regression
```{r}
logMod<-lm(data = d, log(y)~log(x))
logMod
summary(logMod)
ci.slope.log<-confint(logMod, level = 0.9)
ci.slope.log
glogModI<-ggplot(data = d, aes(x = log(x), y = log(y))) + geom_point() + geom_smooth(method = "lm", formula = y ~ x) + stat_poly_eq(formula = logMod, aes(label = paste(..eq.label.., ..rr.label.., sep = "~~~")), parse = TRUE)
glogModI
summary(logMod)
ci <- predict(logMod, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "confidence", 
    level = 0.90)  # CIs for a vector of values
head(ci)
logci.frame<-data.frame(ci)
pi <- predict(logMod, newdata = data.frame(size = d$Brain_Size_Species_Mean), interval = "prediction", 
    level = 0.90)  # for a vector of values
head(pi)
logpi.frame<-data.frame(pi)
logNew<-cbind(r, logci.frame, logpi.frame)
names(logNew) <- c("x", "y", "CIfit", "CIlwr", "CIupr", "PIfit", "PIlwr", "PIupr") #renaming columns
head(logNew)
glogModII<-ggplot(data = d, aes(x = log(x), y = log(y))) + geom_point() + geom_line(data = logNew, aes(x = log(x), y = CIfit), colour = "black") + geom_line(data = logNew, aes(x = log(x), y = CIlwr), colour = "blue") + geom_line(data = logNew, aes(x = log(x), y = CIupr), colour = "blue") + geom_line(data = logNew, aes(x = log(x), y = PIlwr), colour = "red") + geom_line(data = logNew, aes(x = log(x), y = PIupr), colour = "red")
glogModII
```
point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm
```{r}
Mod <- lm(data = n, x ~ y)
pi <- predict(Mod, newdata = data.frame(y = 800), interval = "prediction", 
    level = 0.90)  # for a single value
pi
#Not sure why, but I had to switch x and y on my regression model for the predict function to actually give me an estimate for a single specified value...odd. 
```
Honestly I think my original model was better than my log transformed model. My p values are lower and my R squared values are higher suggesting that my original model is more concise. The points are spread closer to the models fit. 


Five things that emotionally killed me inside:
*How do you write a function...help
*I'm not sure what was meant by interpret the outcome of the test associated with the hypotheses H0: β1 = 0; HA: β1 ≠ 0...I'm sure this one is simple
*I'm an egotistical bitch and I think I'm a legend, but I can't fucking figure out how to add a legend to my goddam ggplot. Like why is this the most difficult ggplot thing to do?
*"Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?" um is this a p value thing cuase its got a lot of wiggle room according to the CIs. 



